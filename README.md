ðŸ“Œ Project Overview

This project performs large-scale analysis of Aadhaar service activity data with a focus on:

Structured data ingestion from monthly CSV dumps

Robust data cleaning and validation

State-level feature aggregation

Month-over-Month (MoM) anomaly detection

Interactive visualization using Streamlit (local execution)

The pipeline is designed to handle high-volume raw data offline, while the Streamlit application serves as a lightweight analytical interface over aggregated outputs.

ðŸŽ¯ Objectives

Build a reproducible data science pipeline for Aadhaar activity analysis

Detect unusual spikes and drops in state-level activity

Provide interpretable, nonâ€“black-box anomaly detection

Demonstrate end-to-end ownership: raw data â†’ insights â†’ visualization

ðŸ“‚ Project Structure
aadhaar_analytics/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                     # Streamlit application
â”‚   â””â”€â”€ geo/
â”‚       â””â”€â”€ india_states.geojson   # India state boundaries (GeoJSON)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Original Aadhaar CSV datasets (NOT committed)
â”‚   â”œâ”€â”€ interim/                   # Intermediate cleaned files
â”‚   â””â”€â”€ processed/                 # Final aggregated datasets
â”‚       â”œâ”€â”€ state_monthly.csv
â”‚       â”œâ”€â”€ state_monthly_anomalies.csv
â”‚       â”œâ”€â”€ master_train.csv
â”‚       â”œâ”€â”€ master_test.csv
â”‚       â””â”€â”€ other derived outputs
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_ingestion.ipynb
â”‚   â”œâ”€â”€ 02_data_cleaning.ipynb
â”‚   â”œâ”€â”€ 03_eda.ipynb
â”‚   â”œâ”€â”€ 04_anomaly_detection.ipynb
â”‚   â”œâ”€â”€ 05_forecasting.ipynb
â”‚   â””â”€â”€ validation.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ merge_csvs.py
â”‚   â”œâ”€â”€ cleaning/
â”‚   â”‚   â””â”€â”€ clean_data.py
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ aggregate_state_monthly.py
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ anomaly.py
â”‚   â”‚   â””â”€â”€ forecast.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ aadhaar_pulse_report.docx
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ðŸ“Š Data Description
Raw Data (Required to Run Full Pipeline)

To reproduce the complete pipeline, you must provide 12 raw CSV files (monthly Aadhaar datasets).

These files are NOT included in the repository and must be placed manually in:

data/raw/

Expected file naming convention (example)
api_data_aadhaar_biometric_500000_1000000.csv
api_data_aadhaar_biometric_1000000_1500000.csv
api_data_aadhaar_biometric_1500000_1810168.csv

api_data_aadhaar_demographic_500000_1000000.csv
api_data_aadhaar_demographic_1000000_1500000.csv
api_data_aadhaar_demographic_1500000_2000000.csv
api_data_aadhaar_demographic_2000000_2071706.csv

api_data_aadhaar_enrolment_500000_1000000.csv
api_data_aadhaar_enrolment_1000000_1006029.csv


âš ï¸ Important

Filenames are expected as-is

Schema must be consistent across files

Large file sizes are expected and handled offline

ðŸ—ºï¸ GeoJSON Requirement (Mandatory)

The Streamlit map requires an India state-level GeoJSON file.

Required file location
app/geo/india_states.geojson

Source (verified & stable)

Download from the following repository:

GeoJSON Source
https://github.com/geohacker/india

Direct file:

states_india.geojson


Rename it to:

india_states.geojson

GeoJSON property used

The application maps states using:

properties.NAME_1


Ensure your CSV state values align with these names.

ðŸ§  Analytical Pipeline
Phase 1: Data Ingestion

Merge multiple monthly CSVs

Schema validation

Deduplication

Phase 2: Data Cleaning

Column normalization

Missing value handling

Date parsing

Phase 3: Feature Engineering

State-level monthly aggregation

Train/test splits

Metric consolidation

Phase 3.5: Visualization Layer

Choropleth map (state-wise)

Metric filters

Temporal slicing

Phase 4: Anomaly Detection

Month-over-Month absolute change

Month-over-Month percentage change

State-wise statistical thresholds

Flags: SPIKE, DROP, NORMAL

ðŸš¨ Anomaly Detection Logic (Summary)

For each state:

Compute MoM percentage change

Calculate state-specific mean and standard deviation

Flag anomalies using:

SPIKE: MoM > mean + 2 Ã— std
DROP : MoM < mean - 2 Ã— std


This approach ensures:

Interpretability

No black-box models

Report-friendly justification

ðŸ–¥ï¸ Streamlit Application (Local)
Purpose

Visual inspection

Insight communication

Demonstration of outputs

âš ï¸ Not designed for large-scale cloud execution

How to Run Locally
1ï¸âƒ£ Create virtual environment
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Ensure required files exist

data/processed/state_monthly.csv

data/processed/state_monthly_anomalies.csv

app/geo/india_states.geojson

4ï¸âƒ£ Run Streamlit
streamlit run app/app.py

ðŸ”’ Version Control & Reproducibility

Entire pipeline is version-controlled using Git

Raw Aadhaar data is intentionally excluded

Repository serves as:

methodological reference

reproducibility artifact

portfolio evidence

âš ï¸ Limitations

Raw datasets are processed offline

Streamlit app uses aggregated outputs only

Cloud deployment is intentionally avoided due to data scale

Forecasting module is experimental and not production-tuned

ðŸ”® Future Enhancements

Cloud-based storage (Parquet + DuckDB)

Automated pipeline orchestration

Advanced anomaly explainability

Policy/event overlay on trends

ðŸ“„ License & Usage

This project is intended for:

academic use

hackathons

portfolio demonstration

Raw Aadhaar data usage must comply with UIDAI data policies.